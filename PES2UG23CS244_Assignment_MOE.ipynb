{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRG5zuk4H7bP",
        "outputId": "4920d409-7dc0-41a8-c054-f1f175f7452c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/138.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m133.1/138.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Groq API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "%pip install groq python-dotenv --upgrade --quiet\n",
        "\n",
        "import os\n",
        "from groq import Groq\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Set up your Groq API Key\n",
        "# If not in your .env file, you'll be prompted to enter it\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    import getpass\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CONFIG = {\n",
        "    \"technical\": {\n",
        "        \"system_prompt\": \"You are a Senior Technical Support Engineer at RocketBoots. Provide rigorous, code-focused, and precise solutions. If code is involved, provide a clean snippet.\",\n",
        "        \"temperature\": 0.7\n",
        "    },\n",
        "    \"billing\": {\n",
        "        \"system_prompt\": \"You are a Billing Specialist at RocketBoots. Be empathetic and professional. Focus on financial accuracy and company refund policies.\",\n",
        "        \"temperature\": 0.7\n",
        "    },\n",
        "    \"general\": {\n",
        "        \"system_prompt\": \"You are a friend. Have a casual conversation with a warm,friendly and empathetic tone.\",\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "pMyrD1F9JJqH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def route_prompt(user_input):\n",
        "    # This instruction is designed to satisfy the \"only category name\" constraint\n",
        "    router_instruction = f\"Classify this text into one of these categories: [technical, billing, general]. Return ONLY the word. Query: {user_input}\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": router_instruction}],\n",
        "        temperature=0 # Consistency is key for routing\n",
        "    )\n",
        "\n",
        "    # Return ONLY the category name as a cleaned string\n",
        "    return response.choices[0].message.content.strip().lower()\n",
        "\n",
        "# Verification: This will show you the output of just the router\n",
        "print(f\"Router Test Output: {route_prompt('My python script is throwing an IndexError on line 5.')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUS96TgBJTCP",
        "outputId": "f69f6293-d101-40db-ec5c-fb748bb56f66"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Router Test Output: technical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_request(user_input):\n",
        "    # 1. Calls route_prompt to decide the category\n",
        "    category = route_prompt(user_input)\n",
        "\n",
        "    # Fallback to general if the router returns something unexpected\n",
        "    if category not in MODEL_CONFIG:\n",
        "        category = \"general\"\n",
        "\n",
        "    print(f\"--- Routing to: {category.upper()} EXPERT ---\")\n",
        "\n",
        "    # 2. Selects the correct System Prompt based on the category\n",
        "    config = MODEL_CONFIG[category]\n",
        "    expert_prompt = config[\"system_prompt\"]\n",
        "\n",
        "    # 3. Calls the generic LLM with specific System Prompt + User Input\n",
        "    # Using Llama 3.1 8B as our base expert model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": expert_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        temperature=config[\"temperature\"] # Experts get higher temp for natural answers\n",
        "    )\n",
        "\n",
        "    # 4. Returns the final answer\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# --- EXECUTION ---\n",
        "# This is where we call the orchestrator to see the final result\n",
        "query = \"I was charged twice for my subscription this month.\"\n",
        "print(\"\\nFINAL ANSWER:\")\n",
        "print(process_request(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuohGPOgJZef",
        "outputId": "392fceec-0131-4974-af48-c10c9ba682d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FINAL ANSWER:\n",
            "--- Routing to: BILLING EXPERT ---\n",
            "I'm so sorry to hear that you've been charged twice for your subscription this month. I can imagine how frustrating that must be for you.\n",
            "\n",
            "Firstly, please know that we take situations like this very seriously, and I'm here to help you resolve the issue as quickly as possible.\n",
            "\n",
            "To assist you further, can you please provide me with your subscription details, including your email address associated with your RocketBoots account, as well as the dates of the duplicate charges? This will enable me to investigate the matter and identify the cause of the error.\n",
            "\n",
            "Once I've located the issue, I'll work with our team to correct the mistake and process a refund for the duplicate charge. I'll also ensure that our system is updated to prevent such errors from happening in the future.\n",
            "\n",
            "In the meantime, if you're experiencing any financial hardship due to the duplicate charge, please let me know, and I'll do my best to provide you with temporary assistance or a payment plan if needed.\n",
            "\n",
            "Your satisfaction and trust in RocketBoots are essential to us, and I'll do everything in my power to rectify this situation and maintain a high level of customer service.\n",
            "\n",
            "Please let me know if there's anything else I can do to support you.\n"
          ]
        }
      ]
    }
  ]
}